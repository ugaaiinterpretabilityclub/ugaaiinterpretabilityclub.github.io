<!DOCTYPE html>
<html lang="en">
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta charset="UTF-8">
    <title>Understanding Our Topic</title>
    <link rel="stylesheet" href="/style.css">
</head>
<body>

<!-- Reusable navbar -->
<div id="navbar"></div>

<main>
    <article class="blog-post">
        <header>
            <h2>Recurrence in Reiforcement Learning</h2>
            <p class="author">By Kyle Jones Â· February 2026</p>
        </header>

        <section>
          <h3>Recurrence in Reinforcement Learning
          </h3>
          <p>I will characterize the human nervus as a totally recurrent reinforcement learning algorithm. This is
not difficult to do since the RL formalism was designed to mimic biological agentics. The human nervus
system can be seen as a vector of activations, where each component of the vector corresponds to sodium
concentrations in a given neuron of the human . For clarity, one neuron per component, one component per
neuron. How do the activations change over time? It depends totally on two things: concentration of sodium in
other neurons and perhaps outside stimulus (maybe donâ€™t include sensory neuron activations in state vector?
Simplifies model but maybe unrealistic) . So, you can characterize a human as a totally recurrent RL agent
with a state vector h encoding both hidden activations and logits. To properly put this into the RL formalism
you would have to compute the probability of h going from one state to any other, since actions happen
exactly when a new state is achieved by h.
          </p>

          <p>People say that we have entered a new age of AI where we â€œlet the brain build itselfâ€ and then do the
exact opposite and sink billions into transformers because they produce good reasoning policies. I think the
most efficient way to allocate compute in an RL agent will come from a totally recurrent agent that can learn
the attention mechanism (or something else) through training.</p>


          <h3>Policy and Action</h3>
          <p>
            I have a goal in life and I intend to attain it, so I take steps towards it. What the hell does that mean?
Step? Action? What do these words really mean? The â€œactions â€ of the human RL agent are infinitesimal muscle
movements (or perhaps changes in muscle activation) in the context of our previous discussion. The agent
that we call ourselves is clearly not operating as the agent of our ideal formalism. Does the proper agent
have within it a proper conception of itself in this way? (now it do) Is the agent we call us a proper agent? Or
is the â€˜agentness â€™ just an illusion â€“ a good way of bookkeeping within the activations of our formal agent?
Could we tell the difference between these two cases?
          </p>

          <p>Out of desperation, lets come up with a list of necessary (but perhaps not sufficient) conditions for the
â€˜agent of self â€™ to be a genuine RL agent : If totally recurrent mass of neurons that is our RL agent is not what
â€˜weâ€™ are to ourselves, then, since we can detect parts of our conscious experience via brain imaging, one
would hope we could classify neurons as apart of â€œus â€ or not. However, even if there was such a distinct
boundary, the outputs of the neural net that was supposedly us is still just neurons.</p>
          <p>Really, the output of the human neural network is the full stack: highest level abstractions down to
fine muscle output and automatic self - regulation. The entire thing as a physical entity that processes
nutrients, abstracts itâ€™s observations, and acts on those abstractions should be what is considered the formal
thing. Then, its clear we exist as whatever is acting on those abstractions. Further, this is not unreasonable
since we are in an ongoing 3 billion year old training run in some sense (maybe)</p>

          <h3>The Training Algorithm</h3>
          <p><b>Claim:</b>Evolution is GRPO with a really weird advantage calculation. <b>Proof:</b> The GRPO objective is
essentially: 
          </p>
          <div style="overflow-x:auto;">
<p>
  <i>J</i>(&theta;) = E[ &sum;<sub>&tau; &in; G<sub>o</sub></sub> 
  ( &pi;<sub>&theta;</sub>(&tau;|o) / &pi;<sub>&theta;<sub>old</sub></sub>(&tau;|o) ) 
  A(&tau;) |<sub>&tau; ~ &pi;<sub>&theta;<sub>old</sub></sub>(&sdot;|o)</sub> ]
</p>
            <p>Where:
            </p>
            <p>
  <i>A</i>(&tau;) = ( mean[r(&tau;) | &tau; &in; G<sub>o</sub>] - r(&tau;) ) / 
  std[r(&tau;) | &tau; &in; G<sub>o</sub>]
</p>
<p>Consider a species of animals on a relatively small island being big enough to house a large population of the
animals. This assumption is essentially respectful English for every animal has the opportunity via colocation
to mate and produce offspring with any other animal in the population . We will further assume that this
species is genderless and every one can make (real, not adopted) babies with everyone else</p>

            <p>Under these assumptions, the act of mate selection will serve as a proxy for advantage in the evolution of the
species. Indeed, if ğ‘Ÿ(ğœ) = #babies made , then a single sample estimate of the objective takes the form:</p>

            <p>
  <i>J</i>(&theta;) &asymp; ( 1 / |G<sub>o</sub>| ) &sum;<sub>&tau; &in; G<sub>o</sub></sub> 
  ( &pi;<sub>&theta;</sub>(&tau;|o) / &pi;<sub>&theta;<sub>old</sub></sub>(&tau;|o) ) 
  [ Z-score, #bbys had by &theta;<sub>old</sub> ]
</p>

            <p>Now, instead of keeping track of only one ğœƒğ‘œğ‘™ğ‘‘, during each gradient step, letâ€™s keep track of |ğºğ‘œ|- many {ğœƒğ‘–}
and stochastically sample the next collection from the gradient we calculate from the estimate:
            </p>
            <p>
  <i>J</i>(&theta;) &asymp; ( 1 / |G<sub>o</sub>| ) &sum;<sub>&tau;<sub>i</sub> &in; G<sub>o</sub></sub> 
  ( &pi;<sub>&theta;</sub>(&tau;<sub>i</sub>|o) / &pi;<sub>&theta;<sub>i</sub></sub>(&tau;<sub>i</sub>|o) ) 
  [ Z-score, #bbys had by &theta;<sub>i</sub> ]
</p>
            <p>
Since this is only one species and they are isolated on an island, it makes sense to draw them IID like this.
Otherwise, we would have to account for subsets of the population evolving independently of the whole, and
this isnâ€™t what im trying to capture here. Though, perhaps you could view the more complicated example as
the case where next generation of agents are drawn from an individually weighted version of the objective.
Perhaps for the agent a with weights ğœƒğ‘, its offspringâ€™s weights would be sampled via SGLD from
            </p>
            <p>
  <i>J</i><sub>a</sub>(&theta;) &asymp; ( 1 / |G<sub>o</sub>| ) &sum;<sub>&tau;<sub>i</sub> &in; G<sub>o</sub></sub> 
  ( &pi;<sub>&theta;</sub>(&tau;<sub>i</sub>|o) / &pi;<sub>a<sub>i</sub></sub>(&tau;<sub>i</sub>|o) ) 
  [ Z-score, #bbys had by &theta;<sub>i</sub> ] * <b>1</b><sub>{a<sub>i</sub> | a had baby with a<sub>i</sub>}</sub>(a<sub>i</sub>)
</p>

            <p>
              Maybe this would mimic the divergent behavior of species in larger populations seen in the real world
            </p>
            <p>Because of our IID assumption, these two quantities are clearly equal in expected value (LLN). Hence, GRPO
can be thought of a â€œflat â€ version of real evolution â€“ kinda. Its unclear to me if the way DNA combines between
humans to produce a new â€“ potentially better â€“ offspring would behave linearly like it does here â€“ but the
principal is g rounded: <b>GRPO is a simple model for evolution among a uniform population of agents.</b> With this
intuition, can we make it better? Does this hint at what kind of tasks â€“ perhaps yet untried â€“ that GRPO might
be good at?</p>
</div>
          
        </section>

        <!-- About the Author Section -->
        <section class="about-author">
            <h3>About Kyle Jones</h3>
            <p>
                Math PhD candidate at Purdue; works in AI Theory
            </p>
        </section>
    </article>
</main>

<!-- Reusable footer -->
<div id="footer"></div>

<script src="/main.js"></script>
</body>
</html>
